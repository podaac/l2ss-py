{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Harmony EOSS L2SS API Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before you start\n",
    "Before you beginning this tutorial, make sure you have an account in the Earthdata Login UAT or Production environment, which \n",
    "will be used for this notebook by visiting [https://uat.urs.earthdata.nasa.gov](https://uat.urs.earthdata.nasa.gov).\n",
    "These accounts, as all Earthdata Login accounts, are free to create and only take a moment to set up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Authentication\n",
    "\n",
    "We need some boilerplate up front to log in to Earthdata Login.  The function below will allow Python\n",
    "scripts to log into any Earthdata Login application programmatically.  To avoid being prompted for\n",
    "credentials every time you run and also allow clients such as curl to log in, you can add the following\n",
    "to a `.netrc` (`_netrc` on Windows) file in your home directory:\n",
    "\n",
    "```\n",
    "machine uat.urs.earthdata.nasa.gov\n",
    "    login <your username>\n",
    "    password <your password>\n",
    "    \n",
    "machine urs.earthdata.nasa.gov\n",
    "    login <your username>\n",
    "    password <your password>\n",
    "```\n",
    "\n",
    "Make sure that this file is only readable by the current user or you will receive an error stating\n",
    "\"netrc access too permissive.\"\n",
    "\n",
    "`$ chmod 0600 ~/.netrc` \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request\n",
    "from http.cookiejar import CookieJar\n",
    "import getpass\n",
    "import netrc\n",
    "import json\n",
    "import requests\n",
    "import sys\n",
    "import shutil\n",
    "import xarray as xr\n",
    "import cmr\n",
    "import numpy as np\n",
    "from podaac.subsetter import subset\n",
    "\n",
    "def setup_earthdata_login_auth(endpoint):\n",
    "    \"\"\"\n",
    "    Set up the request library so that it authenticates against the given Earthdata Login\n",
    "    endpoint and is able to track cookies between requests.  This looks in the .netrc file \n",
    "    first and if no credentials are found, it prompts for them.\n",
    "\n",
    "    Valid endpoints include:\n",
    "        uat.urs.earthdata.nasa.gov - Earthdata Login UAT (Harmony's current default)\n",
    "        urs.earthdata.nasa.gov - Earthdata Login production\n",
    "    \"\"\"\n",
    "    try:\n",
    "        username, _, password = netrc.netrc().authenticators(endpoint)\n",
    "    except (FileNotFoundError, TypeError):\n",
    "        # FileNotFound = There's no .netrc file\n",
    "        # TypeError = The endpoint isn't in the netrc file, causing the above to try unpacking None\n",
    "        print('Please provide your Earthdata Login credentials to allow data access')\n",
    "        print('Your credentials will only be passed to %s and will not be exposed in Jupyter' % (endpoint))\n",
    "        username = input('Username:')\n",
    "        password = getpass.getpass()\n",
    "\n",
    "    manager = request.HTTPPasswordMgrWithDefaultRealm()\n",
    "    manager.add_password(None, endpoint, username, password)\n",
    "    auth = request.HTTPBasicAuthHandler(manager)\n",
    "\n",
    "    jar = CookieJar()\n",
    "    processor = request.HTTPCookieProcessor(jar)\n",
    "    opener = request.build_opener(auth, processor)\n",
    "    request.install_opener(opener)\n",
    "\n",
    "\n",
    "# GET TOKEN FROM CMR \n",
    "def get_token( url: str,client_id: str, user_ip: str,endpoint: str) -> str:\n",
    "    try:\n",
    "        token: str = ''\n",
    "        username, _, password = netrc.netrc().authenticators(endpoint)\n",
    "        xml: str = \"\"\"<?xml version='1.0' encoding='utf-8'?>\n",
    "        <token><username>{}</username><password>{}</password><client_id>{}</client_id>\n",
    "        <user_ip_address>{}</user_ip_address></token>\"\"\".format(username, password, client_id, user_ip)\n",
    "        headers: Dict = {'Content-Type': 'application/xml','Accept': 'application/json'}\n",
    "        resp = requests.post(url, headers=headers, data=xml)\n",
    "        \n",
    "        response_content: Dict = json.loads(resp.content)\n",
    "        token = response_content['token']['id']\n",
    "    except:\n",
    "        print(\"Error getting the token - check user name and password\", sys.exc_info()[0])\n",
    "    return token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find a granule for subsetting\n",
    "\n",
    "Below we call out a specific granule (G1226018995-POCUMULUS) on which we will use the podaac L2 subsetter. Finding this information would complicate the tutorial- but po.daac has a tutorial available for using the CMR API to find collections and granules of interest. Please see the following tutorial for that information:\n",
    "\n",
    "PODAAC_CMR.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "collection = 'C1940473819-POCLOUD'\n",
    "venue = 'prod'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defaults\n",
    "cmr_root = 'cmr.earthdata.nasa.gov'\n",
    "harmony_root = 'https://harmony.earthdata.nasa.gov'\n",
    "edl_root = 'urs.earthdata.nasa.gov'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = cmr.queries.CMR_OPS\n",
    "if venue == 'uat':\n",
    "    cmr_root = 'cmr.uat.earthdata.nasa.gov'\n",
    "    harmony_root = 'https://harmony.uat.earthdata.nasa.gov'\n",
    "    edl_root = 'uat.urs.earthdata.nasa.gov'\n",
    "    mode = cmr.queries.CMR_UAT\n",
    "\n",
    "print (\"Environments: \")\n",
    "print (\"\\t\" + cmr_root)\n",
    "print (\"\\t\" + harmony_root)\n",
    "print (\"\\t\" + edl_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now call the above function to set up Earthdata Login for subsequent requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_earthdata_login_auth(edl_root)\n",
    "token_url=\"https://\"+cmr_root+\"/legacy-services/rest/tokens\"\n",
    "token=get_token(token_url,'jupyter', '127.0.0.1',edl_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Subset of a PO.DAAC Granule\n",
    "\n",
    "We can now build onto the root URL in order to actually perform a transformation.  The first transformation is a subset of a selected granule.  _At this time, this requires discovering the granule id from CMR_.  That information can then be appended to the root URL and used to call Harmony with the help of the `request` library.\n",
    "\n",
    "Above we show how to find a granule id for processing.\n",
    "\n",
    "**Notes:**\n",
    "  The L2 subsetter current streams the data back to the user, and does not stage data in S3 for redirects. This is functionality we will be adding over time.\n",
    "  It doesn't work with URS backed files, which is coming in the next few weeks\n",
    "  it only works on the show dataset, but \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cmr_url = \"https://\"+cmr_root+\"/search/granules.umm_json?collection_concept_id=\"+collection+\"&sort_key=-start_date&token=\"+token\n",
    "response = requests.get(cmr_url)\n",
    "gid=response.json()['items'][0]['meta']['concept-id']\n",
    "print(response.json()['items'][0])\n",
    "print(gid)\n",
    "\n",
    "# Find Bounding box\n",
    "try:\n",
    "    bounding_box = response.json()['items'][0]['umm']['SpatialExtent']['HorizontalSpatialDomain']['Geometry']['BoundingRectangles'][0]\n",
    "\n",
    "    north = bounding_box.get('NorthBoundingCoordinate')\n",
    "    south = bounding_box.get('SouthBoundingCoordinate')\n",
    "    west = bounding_box.get('WestBoundingCoordinate')\n",
    "    east = bounding_box.get('EastBoundingCoordinate')\n",
    "\n",
    "except KeyError:\n",
    "    longitude_list = []\n",
    "    latitude_list = []\n",
    "    polygons = response.json()['items'][0]['umm']['SpatialExtent']['HorizontalSpatialDomain']['Geometry'].get('GPolygons')\n",
    "    lines = response.json()['items'][0]['umm']['SpatialExtent']['HorizontalSpatialDomain']['Geometry'].get('Lines')\n",
    "    if polygons:\n",
    "        for polygon in polygons:\n",
    "            points = polygon['Boundary']['Points']\n",
    "            for point in points:\n",
    "                longitude_list.append(point.get('Longitude'))\n",
    "                latitude_list.append(point.get('Latitude'))\n",
    "            break\n",
    "    elif lines:\n",
    "        points = lines[0].get('Points')\n",
    "        for point in points:\n",
    "            longitude_list.append(point.get('Longitude'))\n",
    "            latitude_list.append(point.get('Latitude'))        \n",
    "    north = max(latitude_list)\n",
    "    south = min(latitude_list)\n",
    "    west = min(longitude_list)\n",
    "    east = max(longitude_list)\n",
    "    \n",
    "#compute a smaller bounding box \n",
    "north = north - abs(.05 * (north - south))\n",
    "south = south + abs(.05 * (north - south))\n",
    "west = west + abs(.05 * (east - west))\n",
    "east = east - abs(.05 * (east - west))\n",
    "\n",
    "\n",
    "longitude = \"({}:{})\".format(west, east)\n",
    "latitude = \"({}:{})\".format(south, north)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Download original granule\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "granule_query = cmr.queries.GranuleQuery(mode=mode)\n",
    "granule_query.format('umm_json')\n",
    "granule_umm_json_url = granule_query.concept_id(gid).token(token)._build_url()\n",
    "response = requests.get(granule_umm_json_url)\n",
    "response_text = json.loads(response.content)\n",
    "urls = response_text.get('items')[0].get('umm').get('RelatedUrls')\n",
    "print(urls)\n",
    "\n",
    "def download_file(url):\n",
    "    local_filename = f\"{collection}_original_granule.nc\"\n",
    "    with requests.get(url, stream=True) as r:\n",
    "        with open(local_filename, 'wb') as f:\n",
    "            shutil.copyfileobj(r.raw, f)\n",
    "    return local_filename\n",
    "\n",
    "for x in urls:\n",
    "    if x.get('Type') == \"GET DATA\" and x.get('Subtype') in [None, 'DIRECT DOWNLOAD'] and '.bin' not in x.get('URL'):\n",
    "        granule_url = x.get('URL')\n",
    "\n",
    "download_file(granule_url) \n",
    "print(granule_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get lon, lat, time variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from netCDF4 import Dataset\n",
    "\n",
    "group = None\n",
    "\n",
    "# Try to read group in file\n",
    "with Dataset(f'{collection}_original_granule.nc') as f:\n",
    "    for g in f.groups:\n",
    "        ds = xr.open_dataset(f'{collection}_original_granule.nc', group=g)\n",
    "        if len(ds.variables):\n",
    "            group = g\n",
    "        ds.close()\n",
    "        \n",
    "#ds = xr.open_dataset('original_granule.nc', engine=\"netcdf4\")\n",
    "if group:\n",
    "    ds = xr.open_dataset(f'{collection}_original_granule.nc', group=group)\n",
    "else:\n",
    "    ds = xr.open_dataset(f'{collection}_original_granule.nc')\n",
    "    \n",
    "lat_var = None\n",
    "lon_var = None\n",
    "\n",
    "# If the lat/lon coordinates could not be determined, use l2ss-py get_coord_variable_names\n",
    "if not lat_var or not lon_var:\n",
    "    try:\n",
    "        lat_var_names, lon_var_names = subset.get_coord_variable_names(ds)\n",
    "        lat_var = lat_var_names[0]\n",
    "        lon_var = lon_var_names[0]\n",
    "    except ValueError:\n",
    "        for coord_name, coord in ds.coords.items():\n",
    "            if 'units' not in coord.attrs:\n",
    "                continue\n",
    "            if coord.attrs['units'] == 'degrees_north':\n",
    "                lat_var = coord_name\n",
    "            if coord.attrs['units'] == 'degrees_east':\n",
    "                lon_var = coord_name\n",
    "try:\n",
    "    time_var = subset.get_time_variable_name(ds, ds[lat_var])\n",
    "except Exception as ex:\n",
    "    time_var = None\n",
    "    \n",
    "print(f'time_var={time_var}')\n",
    "print(f'lat_var={lat_var}')\n",
    "print(f'lon_var={lon_var}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Getting Variables for Collection\n",
    "\n",
    "We use the python-cmr library to query cmr for umm-v associated to the collection. The python-cmr library (https://github.com/nasa/python_cmr) provides us ways to query cmr for different umm data. Then we select a variable that is within the granule to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_query = cmr.queries.CollectionQuery(mode=mode)\n",
    "variable_query = cmr.queries.VariableQuery(mode=mode)\n",
    "\n",
    "collection_res = collection_query.concept_id(collection).token(token).get()[0]\n",
    "collection_associations = collection_res.get(\"associations\")\n",
    "variable_concept_ids = collection_associations.get(\"variables\")\n",
    "\n",
    "if variable_concept_ids is None and venue == 'uat':\n",
    "    print('There are no umm-v associated with this collection and is uat env')\n",
    "    sys.exit(0)\n",
    "\n",
    "original_variable = None\n",
    "\n",
    "if lat_var is None or lon_var is None:\n",
    "    \n",
    "    variables_items = variable_query.concept_id(variable_concept_ids).format('umm_json').get_all()\n",
    "    variables_res = json.loads(variables_items[0]).get('items')\n",
    "    variables = [variable.get('umm').get('Name') for variable in variables_res]    \n",
    "    \n",
    "    for var in variables_res:\n",
    "        var_subtype = var.get('umm').get('VariableSubType')\n",
    "        if var_subtype == \"LONGITUDE\":\n",
    "            lon_var = var.get('umm').get('Name')\n",
    "        if var_subtype == \"LATITUDE\":\n",
    "            lat_var = var.get('umm').get('Name')\n",
    "        if var_subtype == \"TIME\":\n",
    "            time_var = var.get('umm').get('Name')\n",
    "else:\n",
    "    variables_res = variable_query.concept_id(variable_concept_ids[0:50]).get_all()\n",
    "    variables = [variable.get('name') for variable in variables_res]\n",
    "             \n",
    "for x in variables:\n",
    "    try:\n",
    "        if x != lat_var and x != lon_var and x != time_var:\n",
    "            new_path = None\n",
    "            if group:\n",
    "                if group in x:\n",
    "                    new_path = \"/\".join(x.strip(\"/\").split('/')[1:])\n",
    "            \n",
    "            if new_path:\n",
    "                original_variable = x\n",
    "                var_ds = ds[new_path]\n",
    "            else:\n",
    "                var_ds = ds[x]\n",
    "\n",
    "            msk = np.logical_not(np.isnan(var_ds.data.squeeze()))\n",
    "            true_exist = np.all((msk == False))\n",
    "            if true_exist == False:\n",
    "                if new_path:\n",
    "                    variable = new_path\n",
    "                else:\n",
    "                    variable = x\n",
    "                print(f'variable={variable}')\n",
    "                break\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "        pass\n",
    "\n",
    "if ds[variable].size == 0:\n",
    "    print(\"No data in subsetted region. Exiting\")\n",
    "    sys.exit(0)\n",
    "ds.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_variable = variable\n",
    "if original_variable:\n",
    "    subset_variable = original_variable\n",
    "\n",
    "subset_variable = subset_variable.replace('/', '%2F') \n",
    "bboxSubsetConfig = {\n",
    "    'collection_id': collection,\n",
    "    'ogc-api-coverages_version': '1.0.0',\n",
    "    'variable': subset_variable,\n",
    "    'granuleid': gid,\n",
    "    'lat': latitude,\n",
    "    'lon': longitude\n",
    "}\n",
    "bbox_url = harmony_root+'/{collection_id}/ogc-api-coverages/{ogc-api-coverages_version}/collections/{variable}/coverage/rangeset?granuleid={granuleid}&subset=lat{lat}&subset=lon{lon}'.format(**bboxSubsetConfig)\n",
    "print('Request URL', bbox_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with request.urlopen(bbox_url, timeout=240) as response, open(f'{collection}_ogc_temp.nc', 'wb') as out_file:\n",
    "    print('Content Size:', response.headers['Content-length'])\n",
    "    shutil.copyfileobj(response, out_file)\n",
    "    print(f\"Downloaded request to {collection}_ogc_temp.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if group:\n",
    "    ds = xr.open_dataset(f'{collection}_ogc_temp.nc', engine=\"netcdf4\", group=group)\n",
    "else:\n",
    "    ds = xr.open_dataset(f'{collection}_ogc_temp.nc', engine=\"netcdf4\")\n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "print(ds.data_vars)\n",
    "print(len(ds.data_vars))\n",
    "fig, axes = plt.subplots(ncols=3, nrows=math.ceil((len(ds.data_vars)/3)))\n",
    "fig.set_size_inches((15,15))\n",
    "\n",
    "for count, xvar in enumerate(ds.data_vars):\n",
    "    if ds[xvar].dtype == \"timedelta64[ns]\" or ds[xvar].dtype == \"datetime64[ns]\":\n",
    "        continue\n",
    "        #ds[xvar].astype('timedelta64[D]').plot(ax=axes[int(count/3)][count%3])\n",
    "    #ds[xvar].plot(ax=axes[int(count/3)][count%3])\n",
    "    ds[xvar].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify the subsetting worked\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds)\n",
    "var_ds = ds[variable]\n",
    "msk = np.logical_not(np.isnan(var_ds.data.squeeze()))\n",
    "\n",
    "llat = ds[lat_var].where(msk)\n",
    "llon = ds[lon_var].where(msk)\n",
    "\n",
    "lat_max = llat.max()\n",
    "lat_min = llat.min()\n",
    "\n",
    "lon_min = llon.min()\n",
    "lon_max = llon.max()\n",
    "\n",
    "lon_min = (lon_min + 180) % 360 - 180\n",
    "lon_max = (lon_max + 180) % 360 - 180\n",
    "\n",
    "print(lon_min)\n",
    "print(lon_max)\n",
    "print(lat_min)\n",
    "print(lat_max)\n",
    "\n",
    "if (lat_max <= north or np.isclose(lat_max, north)) and (lat_min >= south or np.isclose(lat_min, south)):\n",
    "    print(\"Successful Latitude subsetting\")\n",
    "elif np.isnan(lat_max) and np.isnan(lat_min):\n",
    "    print(\"Partial Lat Success - no Data\")\n",
    "else:\n",
    "    assert False\n",
    "\n",
    "\n",
    "if (lon_max <= east or np.isclose(lon_max,east)) and (lon_min >= west or np.isclose(lon_min, west)):\n",
    "    print(\"Successful Longitude subsetting\")\n",
    "elif np.isnan(lon_max) and np.isnan(lon_min):\n",
    "    print(\"Partial Lon Success - no Data\")\n",
    "else:\n",
    "    assert False\n",
    "\n",
    "ds.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare original Granule to Subsetted Granule\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if group:\n",
    "    original_ds = xr.open_dataset(f\"{collection}_original_granule.nc\", decode_times=False, decode_coords=False, engine=\"netcdf4\", group=group)\n",
    "    subsetted_ds = xr.open_dataset(f\"{collection}_ogc_temp.nc\", decode_times=False, decode_coords=False, engine=\"netcdf4\", group=group)\n",
    "else:\n",
    "    original_ds = xr.open_dataset(f\"{collection}_original_granule.nc\", decode_times=False, decode_coords=False, engine=\"netcdf4\")\n",
    "    subsetted_ds = xr.open_dataset(f\"{collection}_ogc_temp.nc\", decode_times=False, decode_coords=False, engine=\"netcdf4\")\n",
    " \n",
    "for in_var in subsetted_ds.data_vars.items():\n",
    "    #compare attributes\n",
    "    np.testing.assert_equal(in_var[1].attrs, original_ds[in_var[0]].attrs)\n",
    "    \n",
    "    # compare type and dimension names\n",
    "    is_empty = np.isnan(in_var[1])\n",
    "    if not is_empty.all():\n",
    "        assert in_var[1].dtype == original_ds[in_var[0]].dtype\n",
    "        assert in_var[1].dims == original_ds[in_var[0]].dims\n",
    "    else:\n",
    "        print(f\"{in_var[1].name} is empty so skip checking for types\")\n",
    "\n",
    "print(\"Done\")\n",
    "original_ds.close()\n",
    "subsetted_ds.close()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
